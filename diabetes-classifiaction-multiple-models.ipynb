{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.naive_bayes import GaussianNB\nimport numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/predict-diabities'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-03T17:58:03.346402Z","iopub.execute_input":"2023-01-03T17:58:03.347114Z","iopub.status.idle":"2023-01-03T17:58:05.011905Z","shell.execute_reply.started":"2023-01-03T17:58:03.347010Z","shell.execute_reply":"2023-01-03T17:58:05.010082Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/predict-diabities/diabetes.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/predict-diabities/diabetes.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:58:09.082587Z","iopub.execute_input":"2023-01-03T17:58:09.083066Z","iopub.status.idle":"2023-01-03T17:58:09.104087Z","shell.execute_reply.started":"2023-01-03T17:58:09.083030Z","shell.execute_reply":"2023-01-03T17:58:09.102585Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()\ndf.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for duplicated rows","metadata":{}},{"cell_type":"code","source":"for i in df.columns:\n    print(df[i].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df.corr()\ncorr.style.background_gradient(cmap = 'coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for correlation among features and target variable","metadata":{}},{"cell_type":"code","source":"y= df.Outcome\ndf.drop(columns = ['Outcome'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:58:14.639111Z","iopub.execute_input":"2023-01-03T17:58:14.639629Z","iopub.status.idle":"2023-01-03T17:58:14.659724Z","shell.execute_reply.started":"2023-01-03T17:58:14.639591Z","shell.execute_reply":"2023-01-03T17:58:14.657925Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ros = RandomOverSampler(random_state=0)\nX, y = ros.fit_resample(df, y)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:58:18.027061Z","iopub.execute_input":"2023-01-03T17:58:18.027596Z","iopub.status.idle":"2023-01-03T17:58:18.041732Z","shell.execute_reply.started":"2023-01-03T17:58:18.027549Z","shell.execute_reply":"2023-01-03T17:58:18.040160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Balancing Class\nas we have many values of a class and few valuesof others, we will balance the classes using OverSampling to make the classes balanced","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nscaled = sc.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:58:21.604091Z","iopub.execute_input":"2023-01-03T17:58:21.604601Z","iopub.status.idle":"2023-01-03T17:58:21.616984Z","shell.execute_reply.started":"2023-01-03T17:58:21.604564Z","shell.execute_reply":"2023-01-03T17:58:21.615474Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Standardizing the features","metadata":{}},{"cell_type":"code","source":"m1 = IsolationForest()\nm1.fit(df)\ndf['anamoly'] = m1.predict(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Isolation forest to separate the outliers. not used here","metadata":{}},{"cell_type":"code","source":"trainx,testx,trainy, testy = train_test_split(scaled, y, test_size =.35, random_state = 100)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:58:29.157058Z","iopub.execute_input":"2023-01-03T17:58:29.157545Z","iopub.status.idle":"2023-01-03T17:58:29.165550Z","shell.execute_reply.started":"2023-01-03T17:58:29.157508Z","shell.execute_reply":"2023-01-03T17:58:29.164224Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Splitting the train set into train and test","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\npenalty = ['l1', 'l2', 'elasticnet']\nc = np.array([0.001, 0.01, 0.1, 1, 10, 100])\nsolv =  ['lbfgs','liblinear','newton-cg', 'newton-cholesky','sag','saga']\nmax_iter = np.array(range(100,1000))\ngrid = {'penalty': penalty, 'C': c, 'solver' : solv, 'max_iter': max_iter}\ngridlogistic = GridSearchCV(model, grid, cv = 5)\ngridlogistic.fit(trainx, trainy)\nprint('penalty', gridlogistic.best_estimator_.penalty)\nprint('C', gridlogistic.best_estimator_.C)\nprint('solver', gridlogistic.best_estimator_.solver)\nprint('max_iter', gridlogistic.best_estimator_.max_iter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"many fits failed because not all penalty are compatible with solver. Thus not a major impact using grid or random search\n297000 fits failed out of a total of 486000.\n","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(penalty = 'l2', C = 0.1 , solver = 'saga', max_iter = 648,  random_state = 100)\nmodel.fit(trainx, trainy)\nmodel.score(testx, testy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Used LogisticRegression The accuracy was 75%","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nmin_split = np.array([2, 3, 4, 5, 6, 7])\nmax_nvl = np.array([3, 4, 5, 6, 7, 9, 11])\nalg = ['entropy', 'gini']\nnest = np.array(range(1,101))\nvalues_grid = {'min_samples_split': min_split, 'max_depth': max_nvl, 'criterion': alg, 'n_estimators': nest}\ngridRandomTree = GridSearchCV(estimator = model, param_grid = values_grid, cv = 5)\ngridRandomTree.fit(trainx, trainy)\nprint('Mín Split: ', gridRandomTree.best_estimator_.min_samples_split)\nprint('Max Nvl: ', gridRandomTree.best_estimator_.max_depth)\nprint('Algorithm: ', gridRandomTree.best_estimator_.criterion)\nprint('n_estimators', gridRandomTree.best_estimator_.n_estimators)\nprint('Score: ', gridRandomTree.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(criterion = 'entropy', min_samples_split = 2,  max_depth = 9, n_estimators = 34, random_state = 100)\nmodel.fit(trainx, trainy)\nmodel.score(testx, testy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Used random forest The accuracy was 82%","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier()\nmin_split = np.array([2, 3, 4, 5, 6, 7])\nmax_nvl = np.array([3, 4, 5, 6, 7, 9, 11])\nalg = ['entropy', 'gini']\nvalues_grid = {'min_samples_split': min_split, 'max_depth': max_nvl, 'criterion': alg}\ngridDecisionTree = GridSearchCV(estimator = model, param_grid = values_grid, cv = 5)\ngridDecisionTree.fit(trainx, trainy)\nprint('Mín Split: ', gridDecisionTree.best_estimator_.min_samples_split)\nprint('Max Nvl: ', gridDecisionTree.best_estimator_.max_depth)\nprint('Algorithm: ', gridDecisionTree.best_estimator_.criterion)\nprint('Score: ', gridDecisionTree.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier(criterion = 'gini', min_samples_split = 3, max_depth= 11, random_state= 100)\nmodel.fit(trainx, trainy)\nmodel.score(testx, testy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Used DecisionTree The accuracy was 79%","metadata":{}},{"cell_type":"code","source":"level_0_estimators = dict()\nlevel_0_estimators[\"logreg\"] = LogisticRegression( random_state=100)\nlevel_0_estimators[\"forest\"] = RandomForestClassifier(criterion = 'entropy', min_samples_split = 2,  max_depth = 9, n_estimators = 34, random_state = 100)\n \nlevel_0_columns = [f\"{name}_prediction\" for name in level_0_estimators.keys()]\n \nlevel_1_estimator = RandomForestClassifier(criterion = 'entropy', min_samples_split = 2,  max_depth = 9, n_estimators = 34, random_state = 100)\n\nkfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 100)\nmodel = StackingClassifier(estimators=list(level_0_estimators.items()), \n                                    final_estimator=level_1_estimator, \n                                    passthrough=True, cv=kfold, stack_method=\"predict_proba\")\n\nmodel.fit(trainx, trainy)\nmodel.score(testx, testy)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T18:01:02.010169Z","iopub.execute_input":"2023-01-03T18:01:02.010596Z","iopub.status.idle":"2023-01-03T18:01:03.108137Z","shell.execute_reply.started":"2023-01-03T18:01:02.010562Z","shell.execute_reply":"2023-01-03T18:01:03.106811Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0.8457142857142858"},"metadata":{}}]},{"cell_type":"markdown","source":"Using stacking classifier yielded the best accuracy 85% ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}